\documentclass{book}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{stmaryrd}

\begin{document}

\newtheorem{puz}{Puzzle}[chapter]


\title{Mathematical Structure for Programming}
\author{Conor McBride}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{spuds and a lemon}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let's make some numbers! You'll need a bag of lemons, a sack of potatoes, and a load of sticks (e.g. cocktail sticks) which are pointy at both ends. (If you have suitable electrodes and wires, you can make batteries as well as number.)

There are two ways to get a number in your hand:
\begin{enumerate}
\item grab a lemon---it's a number all by itself;
  \item if you already have a number in your hand, stab the thing in your hand with a stick (don't stab your hand), then press a potato onto the other end of the stick and hold onto that potato instead
\end{enumerate}

\newcommand{\spud}{\mbox{spud}\!\!\!-\!\!\!}
\newcommand{\lemon}{\mbox{lemon}}
So that means we can make (with your hand on the left)
\[\begin{array}{l}
\lemon\\
\spud\lemon\\
\spud\spud\lemon\\
\spud\spud\spud\lemon\\
\vdots
\end{array}\]

(Incidentally, a crucial ``let's pretend'' in this game is that you can tell potatoes apart from lemons, but that blind to any difference between two potatoes or between two lemons.)

Now, if you only make numbers this way, starting only with a lemon and holding onto only the most recently joined potato, your numbers will all have an important property. \textbf{If you follow the sticks down the number, you will eventually get to the lemon.} No sneaky moves, like making a circular model from potatoes and sticks, or always adding on the next potato just in time.

If you're thinking ``Those aren't numbers: that's just sculpture with groceries!'', then I have two things to tell you.
\begin{enumerate}
\item Our silly models with potatoes and lemons held together by sticks are as valid a representation of numbers as $5$ or MMXXIV.
\item Count the spuds!
\end{enumerate}
But how do you know you \emph{can} count the spuds? How do you know that you won't just keep counting for ever? It's because you eventually get to the lemon.


\section{addition as substitution}

Adding these numbers is easy. If you're holding one of them in each hand, work your way along the left number until you reach the left lemon, then detach it, and stab the right number with the exposed stick, then work your way back out to the leftmost spud. In other words, \emph{substitute} the right \emph{number} for the left \emph{lemon}
\[\begin{array}{l@{\;\,}l}
\spud\spud\spud\spud\spud&\lemon\\
&+ \\
&\spud\spud\spud\spud\spud\spud\lemon\\
&= \\
\spud\spud\spud\spud\spud&\spud\spud\spud\spud\spud\spud\lemon\\
\end{array}
\]
Note that the effectiveness of this procedure relies on the fact that you are guaranteed to find the left lemon.

It's also worth asking these three things:
\begin{enumerate}
\item What if the number in your left hand is a lemon? (You replace the lemon in your left hand with the number in your right, and the answer is exactly the right number.)
\item What if the number in your right hand is a lemon? (You replace the left lemon by the lemon in your right hand and the answer is indistinguishable from the left number you started with.)
\item What if you add three things? Does it matter whether you add left to middle first, or middle to right? (It doesn't matter. Both ways round, you end up with all the spuds you started with, in the same order (not that you can tell), and the rightmost lemon on the end.)
\end{enumerate}


\section{synchronized spuds make a race to the lemon}

Suppose you are holding two numbers (left and right, say), each by their most recent potato. What will happen if you work your way towards the two lemons in tandem, one potato per step in each? Remember that for both numbers, you are sure that stepping in this way means you will eventually get to the lemon? It's a race to the lemon! There are three possibilities:
\begin{itemize}
\item you reach the left lemon while the right number still has a potato. That means the original left number was strictly smaller than the original right number, and now that the left number is a lemon, the current right number is the \emph{difference}. As it were, you have found out that the original numbers were $x$ and $x+\spud y$ for some $x$ and $y$;
\item you reach the left lemon and the right lemon at the same time --- the dead heat means the numbers were both some $x$;
\item you reach the right lemon while the left number still has a potato. So the original left number was greater and the current left number is by how much. The original numbers were $x+\spud y$ and $x$, for some $x$ and $y$.
\end{itemize}

You should notice that, in all three cases, the number which won the lemon race, called $x$ above, is the largest number from which you can reach \emph{both} numbers by \emph{adding}. You either add $\lemon$ and $\spud y$, $\lemon$ and $\lemon$, or $\spud y$ and $\lemon$, respectively. You have figured out how close you can get to both numbers by adding, together with what the differences are. In other words, we're \emph{undoing} adding by as little as possible. Here, at least one of the differences is nothing, and when the inputs are equal they both are.

It's funny: this lemon race tells you `maximum' and `minimum', `less than, equal, or greater than', and `difference' all at once. The usual mathematical or programming presentation makes all of these tests and operations distinct, but they are all incomplete facets of the same underlying computation. (The same is true of computations in hardware using binary numbers.)


\section{Euclid's lemon racing game}

Here's a game that the Greek mathematician Euclid used to play with potatoes and lemons.\footnote{Consider a pinch of historical salt.} It's a game which computes one number from two.

You start with two numbers, one in each hand. If one hand holds a lemon, stop: the answer is the number in your other hand! Otherwise, run a lemon race. Now, at least one hand is holding a lemon: put that hand back to the start of its number; keep the other hand where it is and detach any potatoes you passed by to get there. That's to say, replace the larger number by the difference. Go back to the start.

Why does this game reach an end? Because you don't run the lemon race unless you start with potatoes in both hands, so each step will result i one or other hand moving strictly nearer its lemon, and both sides promise that you eventually get to the lemon.

But, perhaps more importantly, what on earth is the meaning of the output of this game?

(Some people have been known to tell a version of this story in which Euclid repeatedly breaks the largest square possible off from a rectangular piece of chocolate\footnote{Likewise.} until there is no chocolate left. I find the spuds and lemons version more savoury, because nobody wants a rectangle of chocolate with width zero.)


\section{puzzles}

\begin{puz}
  An `add-respects-or' test is a computable way to decide a true-or-false property of spuds-and-lemon numbers such that
  \begin{itemize}
  \item the test gives false for a $\lemon$;
    \item whenever you can split up any number $x$ as $y+z$ in any way, the test gives true for $x$ if and only if the test gives true either for $y$ or for $z$ or for both.
  \end{itemize}
  A `boring' test is one which always gives the same answer, no matter what number you test.
  \begin{enumerate}
  \item Find one boring add-respects-or test.
  \item Either find a different boring add-respects-or test, or explain why no such test exists.
  \item Find an add-respects-or test which is \emph{not} boring.
  \item Either find a different non-boring add-respects-or test, or explain why no such test exists.
  \end{enumerate}
(To show that two tests are different, you must be able to explain why there is some number where they disagree.)
\end{puz}

\begin{puz}
 Let us say that a number is `munky' if it is exactly divisible by three, `minky' if division by three leaves a remainder of one, and `manky' if division by three leaves a remainder of two. Explain how to test these properties of spuds-and-lemon numbers. You should be able to remove one spud at a time and test what is left. After all, you will eventually get to the lemon! Hint: it is easier to solve all three parts of this puzzle together, rather than taking the one at a time.
\end{puz}

\begin{puz}
Our method of addition replaced the left $\lemon$ by the right number. You could also consider computations which replace each $\spud$ in a number by the same something (which had better be ready to attach to a number). What can you compute in this way?
\end{puz}

\begin{puz}
Our method of addition dismantles the left number and keeps the right number intact. It is thus far from obvious that $x+y = y+x$. Devise a method for adding in a way which is conspicuously symmetrical. Do the same for multiplication.
\end{puz}

\begin{puz}
  A `multiply-respects-or' test is a computable way to decide a true-or-false property of spuds-and-lemon numbers such that
  \begin{itemize}
  \item the test gives false for $\spud\lemon$;
  \item whenever you can split up any number $x$ as $y\times z$ in any way, the test gives true for $x$ if and only if the test gives true either for $y$ or for $z$ or for both.
  \end{itemize}
  \begin{enumerate}
  \item Are any of your `minky', `manky', `munky' tests multiply-respects-or? If so, which and why?
  \item In a black box, I have a multiply-respects-or test which is not boring. What answer does it give for the $\lemon$? No, you may not look in the box!
  \item Is divisibility by \emph{four} a multiply-respects-or test? Why?
  \item What are the non-boring multiply-respects-or tests? (Note: it's terribly unfair of me to ask this question at this stage, but we'll make sense of it eventually.)
  \end{enumerate}
\end{puz}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{seeing the trees}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Computer programs are written in formal languages. You can't just say any old thing and expect a computer to make sense of it, even if these days they're quite willing to bullshit you that they understand. Computer programs tend to be written textually, as a sequence of \emph{lines}, each of which is a sequence of \emph{characters} (i.e., letters, digits, spaces, punctuation, emoji, etc), and that's not particularly helpful if you're trying to understand them---it's merely a convenient fit with our ancient methods for writing stuff down. Text doesn't have much obvious structure, but computer programs do. We need to learn to see, and to talk about how to see.

We have a language (the language of \emph{grammars}) for talking about languages. Let me start with an example
\newcommand{\nt}[1]{\langle\mathit{#1}\rangle}
\[\begin{array}{rrl}
\nt{Number} & ::= & \lemon\\
            &   | & \spud\nt{Number}
\end{array}\]
How to read this? Where to begin? Which of these things is nearest to hand? You can't be expected to know until someone establishes the conventions.

The place to start is with $::=$, which you can pronounce `is defined to be'. It's a variation on the theme of $=$, but with a particular spin. You're not being asked to agree with this equation, and there's no way you can check it. You're not supposed to think (or pretend) `Oh I knew that!'. When I write this equation, I'm telling you something \emph{new}, and asking you to put up with it for the time being.

The thing to the left of $::=$ is the name of a new \emph{sort of thing}---by convention, the names of sorts of things are written in $\nt{\cdot}$ to distinguish them from \emph{actual} things. The $\nt{\cdot}$ does the job of `a' or `an' in English---the indefinite article---we're talking about a generic class of things by imagining one, but not a special one. What I'm saying is `I'm inventing a new sort of thing, namely a $\mathit{Number}$, and I'm telling you of what a $\mathit{Number}$ may consist.'. Everything after $::=$ is the explanation of what I allow these $\mathit{Number}$s to be.

The next thing to pay attention to is the $|$ which you can read as `or'. I'm giving you a choice of ways to make $\mathit{Number}$s.

The first choice I offer is `$\lemon$'. See? In English prose I write it in quotation marks, because I mean the actual word `$\lemon$'. It's not a \emph{variable} which could stand abstractly for a variety of things. It's concretely what it says.

The second choice I offer is the concrete symbol `$\spud{\,}$' which again (because no $\nt{\cdot}$) means only what it says, followed by $\nt{Number}$. The latter indicates that any $\mathit{Number}$ you have made already can go in that place.

I didn't offer any other choices. Implicit in this definition is that it is \emph{exhaustive}. Spuds and lemons, that's your lot! I'm saying ``I'm inventing a new sort of thing, namely a $\mathit{Number}$, which consists either of `$\lemon$' or of `$\spud{}$' followed by another $\mathit{Number}$.'' and I'm also saying ``You can't possibly have \emph{finished} making a $\mathit{Number}$ unless you eventually get to a `$\lemon$'.''.

So what are these $\mathit{Number}$s?
\[\begin{array}{r}
\lemon \\
\spud\lemon \\
\spud\spud\lemon \\
\spud\spud\spud\lemon \\
\vdots
\end{array}\]
But these examples and the `and so on' indicated by `$\vdots$', don't tell us precisely what $\mathit{Number}$s are, in general, whereas the grammar does.

\newcommand{\nta}[2]{\begin{array}[b]{l} #2\\ \uparrow\\ \nt{#1}\end{array}}
The grammar does another useful thing, as well. It tells us how to perceive a $\mathit{Number}$ as a structure. We can draw a picture of how to see $\spud\spud\lemon$ as a $\mathit{Number}$:
\[\nta{Number}{\spud\nta{Number}{\spud\nta{Number}{\lemon}}}\]
I've drawn this picture, which is called a `parse tree', with the bottom nearest to hand (they're sometimes drawn the other way up), so you should read it bottom-to-top, like a detective, even though time went top-to-bottom\footnote{In the beginning was the $\lemon$\ldots}. It records how the choices offered by the grammar can be employed to make sense of the text.

Old computer scientists \emph{see} these trees when we \emph{look} at text. We don't even notice that we're doing it, but we experience distress when it doesn't just happen automatically and we look for someone else to blame. You are not born with this mode of perception, and it may take some practice to acquire it.


\section{what's in a grammar (and what isn't)?}

\newcommand{\X}{\mathsf{X}}
\newcommand{\SB}{\mathsf{SumBelow}\;}
\newcommand{\SH}{\mathsf{Shift}\;}
Let's have another grammar with a little more to it. It's a little language of formulae involving one variable $\X$.
\[\begin{array}{rrl}
\nt{Formula}
& ::= & \X \\
&   | & \nt{Number} \\
&   | & \nt{Formula} * \nt{Formula} \\
&   | & \nt{Formula} + \nt{Formula} \\
%&   | & \SB \nt{Formula} \\
%&   | & \SH \nt{Formula} \\
&   | & (\nt{Formula})
\end{array}\]

What do we notice? Firstly, we have two ways to make a formula with no subformulae---the variable, $\X$ and any number you like. We could insist that numbers be written in $\spud\lemon$ form, but let's allow ourselves the brevity of Arabic numerals. That means we're in a position to make an `eventually reaching the $\lemon$' promise about formulae: if you keep tearing formulae apart, you eventually reach $\X$ or a number (in which you eventually reach the $\lemon$).

Secondly, we have choices with \emph{two} subformulae. That means our parse trees can spread out like they're called trees for a reason. E.g., $(\X + 2)*\X$ has parse tree
\[
\nta{Formula}{(\nta{Formula}{\nta{Formula}{\X} + \nta{Formula}{\nta{Number}{2}}})}
*
\nta{Formula}{\X}
\]
If we're exploring the parse tree, we have a choice of paths we can take. Our `eventually $\lemon$' promise has to get stronger: you eventually reach $\X$ or a number, \emph{whatever path you choose}.

Thirdly, I haven't said anything about what these formulae mean. You might \emph{think} you know what $+$ and $*$ mean, because they look like standard symbols for addition and multiplication, but I haven't confirmed that that's what \emph{I} mean by them. Grammars don't say what things mean---they filter and structure texts which are \emph{plausible}, discarding texts which are obvious \emph{gibberish}. That's to say they \emph{ask} helpful questions about the meanings of some things in particular, but they don't offer any \emph{answers}.

Fourthly, the grammar is \emph{ambiguous}. It is not immediately obvious whether $2+\X*\X$ says
\[
\nta{Formula}{\nta{Formula}{2}+\nta{Formula}{\nta{Formula}{\X}*\nta{Formula}{\X}}}
\]
or
\[
\nta{Formula}{\nta{Formula}{\nta{Formula}{2}+\nta{Formula}{\X}}*\nta{Formula}{\X}}
\]
Worse, we do not automatically know whether $\X+\X+\X$ groups to the left or to the right, and we might hope not to care, but for now we must worry about it. You may have learned some rules at school about this sort of thing, and that's good, because at least you can see we might need rules to manage this sort of situation. But I'm a \emph{university} teacher, so I can take school rules, grind them to a powder and blow them out the window, if I feel that would improve my day.
The simplest strategy to deal with ambiguity is to reject ambiguous texts, given that our grammar lets us use parentheses to resolve them: at least, that's what we might \emph{hope} the parentheses are for. At the moment, we have no reason to believe $\X$, $(\X)$ and $((\X))$ will all mean the same thing, and there are computer languages where wrapping extra sets of parentheses round stuff really does change its meaning.

As a side remark, one awkward consequence of mathematics being taught at school to some fairly standardised curriculum is a misplaced entitlement to notation working in all sorts of contexts the way it did in that context. One symptom of this malaise is the presumption that parentheses $(\cdot)$, brackets $[\cdot]$ and braces $\{\cdot\}$ are interchangeable and serve only to indicate grouping. This is very seldom, if ever, the case in computer programming languages, or in the notations we use to write about them. You should never expect $[x]$ to mean the same as $x$.

For \emph{this} particular language of formulae, I'm willing to promise you that parentheses $(\cdot)$ are used only for grouping, i.e. to choose between possible parse trees. The meaning of a formula with enclosing parentheses will always be the same as that formula without parentheses. I promise you that
\[\nta{Formula}{(\nta{Formula}{\nta{Formula}{\X}+\nta{Formula}{2}})}
\qquad\mbox{and}\qquad
\nta{Formula}{\nta{Formula}{\X}+\nta{Formula}{2}}
\]
with the parenthesesized version needed only to avoid ambiguity when this formula is part of a larger formula.

The notion of grammar we have here is often called `context-free'. The rules for what constitutes a formula don't change, depending on where the formula is. There are more complex aspects to meaningfulness which context-free grammars can't capture. For example, if we wanted to write the grammar of grammars themselves, we could not enforce the property that every sort of thing mentioned to the right of $::=$ has a unique definition, i.e., occurring to the left of exactly one $::=$ somewhere in the same grammar. That's to say the \emph{scope} of names is exactly the sort of context which context-free grammars can't handle. So, don't expect grammars to encode complicated requirements for coordination between separate regions of a text.

Grammars give us a way to specify languages which are local, artificial, and closed. Out there, in the wilds of mathematics, there might be subtraction and division and $\pi$ and $\mathsf{Y}$, but when we choose to invent what a $\nt{Formula}$ is in this particular way, we can put all of those cosiderations to one side. There's a certain sense of ``Here, we need only worry about\ldots'' with grammars. Of course, that cuts both ways: I'm within my rights to invent grammars which are local to one puzzle. You cannot simply memorise all the grammars you will ever need. That would be like learning a poem by heart innstead of learning to listen.


\section{a note on recursion}

Both of the grammars we've seen so far have been \emph{recursive}. We've said how to make bigger numbers from smaller numbers, and how to make bigger formulae from smaller formulae. Recursion often makes beginners nervous---circularity is suspicious---I'm not going to make the mistake of playing down that nervousness, in an `Oh, you'll get used to it!' sort of way. I'm here to tell you that it's quite reasonable to be nervous about recursion, and to help you ask the extra questions you will need to reassure yourself about recursions you may encounter in the wild.

For a start, it helps to make a distinction between two ways in which recursion can make sense. Some recursions are trying to achieve some functionality which are \emph{eventually finished}: these rely on each recursive sub-thing being simpler, with some guarantee that things can't keep getting simpler forever---you eventually stop because you eventually reach a `base case' with no subproblems. Other recursions are trying to achieve some functionality which is \emph{always ready}: it might stop, it might keep going, but it will always let you ask `what's next?'. The latter is how you expect computer games to behave: you might win, you might lose, but you might just keep on going, and you expect the game to keep responding to you.

The bad recursions never finish but can become unresponsive, and that can happen. Those recursions are worth being afraid of. If you see something defined by recursion, don't run away, but don't assume it makes sense. See if you can find out why it's eventually finished or always ready.

Now, in our world of grammars, we expect our parse trees to have the property that following any path through them eventually stops. What can we say about Theresa May's famous grammar?
\[\begin{array}{rrl}
\nt{Brexit} & ::= & \nt{Brexit}
\end{array}\]
Is it a bad grammar? No, merely \emph{pointless}! It is well constructed. There is one choice of how to make a Brexit: you make it from a Brexit. So there are no parse trees for this grammar with finite paths, and no text can possibly make sense as a Brexit.


\section{abstract syntax trees}

\newcommand{\D}[1]{\mathtt{#1}}
\newcommand{\C}[1]{\mathtt{#1}}

Grammars tell us how to see structure in a text. A parse tree shows the original text with that structure imposed upon it. If we want to compute with a formula, perhaps to give it some sort of meaning, it is this structure which matters, not the precise details of the text. We should hope that it doesn't matter\footnote{but I could tell you some horror stories} whether you write $\X+2$ or $\X\;+\;2$, and a few extra sets of parentheses shouldn't make any difference, provided they go round valid subformulae. How do we get our hands on the structure?

We could, of course, make \emph{models} of these structures from fruit, vegetables, and cocktail sticks. We might use an apple with two sticks coming out to build a formula with $+$, a pear with two sticks for $*$, a nectarine for a numerical formula (with one stick to connect it to a spuds-and-lemon number), and a sprout with no sticks coming out for $\X$. Then we can compute with formulae in the same hand-on way we did with numbers. You always start holding the outermost thing, but you can follow the sticks inwards, ad no matter which way you go, you will eventually get to a nectarine or a sprout.

The nectarines are important. Conceptually, formulae are distinct from numbers. They are different \emph{types} of thing. If we're holding on to a formula, we expect an apple, a pear, a nectarine or a sprout. If we're holding on to a number, we expect a spud or a lemon. It is visually appealing to write a numerical formula as just the unadorned number itself. But when we're explaining computational processes, it will be helpful to be precise about when we are working with numbers and when we are working with formulae. The number 2 is made from two spuds and a lemon; the formula 2 is made from a nectarine, two spuds and a lemon.

Note that the apples and pears are ways of making a tree for a formula, just as spuds and lemons make numbers, whereas addition and multiplication are processes we know for \emph{computing} with numbers. We could \emph{interpret} a formula as a calculation to perform with numbers:
\begin{itemize}
\item to interpret an apple formula as a number, interpret both subformulae as numbers, then add those numbers;
\item to interpret a pear formula as a number, interpret both subformulae as numbers, then multiply those numbers;
\item to interpret a nectarine formula as a number, remove the nectarine and give back the number it's connected to.
\end{itemize}
You can see that the above procedure is recursive, but it's the `eventually finished' kind of recursion: it works by taking the formula apart into smaller and smaller pieces. A formula written `$2*3$' turns into
\[\begin{array}{l@{\quad}l}
\spud\spud\lemon & \spud\spud\spud\lemon \\
| & | \\
\mbox{nectarine} & \mbox{nectarine} \\
\multicolumn{2}{c}{\backslash\mbox{pear}/\hspace*{0.7in}}
\end{array}\]
Our procedure tells us first to peel off the nectarines to give us $2$ and $3$ respectively, and then to multiply those numbers, yielding 6.

But haven't I forgotten something? What if the formula says `$2*\X$'? What on earth are we to do with the sprout? What number shall we return when we see it?\footnote{decem?} We won't know unless somebody tells us. Let us make a virtue of that necessity. To do this procedure properly, we need to be given two inputs, a formula and the number that $\X$ stands for. Now we can give the missing rule:
\begin{itemize}
\item to interpret a sprout formula, copy the number that was given as an input.
\end{itemize}

So if the formula is `$2*\X$' and the number is 7, we multiply 2 by 7 to compute the output 14.


\section{substitute the sprouts}

Remember when we did adding? We had a left number and a right number, and we replaced the left lemon by the right number. Guess what! We can do something very similar with two formulae.

Suppose you have a left formula and a right formula. You can make a new formula by replace \emph{all} the left sprouts by copies of the right formula. So if we have `$\X*\X$' on the left and `$\X+2$' on the right, we'll get `$(\X+2)*(\X+2)$' as our new formula. Note that I had to put some extra parentheses in the text of the new formula to make sure it parses unambiguously as the tree I computed: the substitution was not textual but structural. Bear in mind also that our formula trees might not have any sprouts in them: if the left formula is `$5$' and the right formula is `$\X+2$', we'll get $5$ as the result of substituting all of the left sprouts --- all none of them.

It's also worth asking these three things:
\begin{enumerate}
\item What if the formula in your left hand is a sprout? (You replace the sprout in your left hand with the formula in your right, and the answer is exactly the right formula.)
\item What if the formula in your right hand is a sprout? (You replace all the left sprouts by the copies of the sprout in your right hand and the answer is indistinguishable from the left formula you started with.)
\item What if you substitute three things? Does it matter whether you substitute middle into left first, or right into middle? (It doesn't matter. Both ways round, you end up with the same overall combined formula.)
\end{enumerate}

Surprised? You shouldn't be. Substituting all the sprouts is a lot like substituting all the lemons. It's just that the way numbers are built means that `all the lemons' means `exactly the lemon you're bound to find at the end', whereas a formula can contain any number (including zero) of sprouts.

Here's something that might seem a little more surprising. Let's take formulae `$\X*\X$' on the left and `$\X+2$' on the right, again and consider the number $5$. We could
\begin{itemize}
\item either interpret `$\X+2$' with $5$ for $\X$ and get $7$, then interpret `$\X*\X$' with $7$ for $\X$ and get $49$.
\item or substitute to get `$(\X+2)*(\X+2)$', then interpret with $5$ for $\X$ to get $49$.
\end{itemize}
Coincidence? I think not! There's some connection between substituting the formulae and sequencing their interpretations, and it has something to do with the way substituting with a sprout in either hand means `do nothing' and interpreting a sprout means `do nothing' to the number we start with. Does it always work? I could say `yes', but I might be lying. And you could try some experiments, but then you might just get bored before you get unlucky. Let's learn to \emph{prove} it.

(Also, let's see if we can build trees out of something more manageable than fruit and veg.)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{squish a bunch of stuff}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

If you tap your foot five times, and then you tap your foot six times, you'll have tapped your foot eleven times. There's some kind of relationship between adding up numbers and repetitive action sequences.

If you have one bag of five potatoes and another of six, combining both bags into one will give you a bag of eleven potatoes. Bags of spuds don't behave this way because they were taught arithmetic at school. Arithmetic behaves this way because it helps us think about bags of spuds.

If you multiply two by five, that's ten; and two times six is twelve. Adding ten and twelve gives twenty-two, which is twice eleven. There's some kind of relationship between adding up numbers and adding up their doubles. (You may have heard this called ``The Distributive Law'', as if there were only one.)

If you raise two to the power five, that's thirty-two; and two to the six is sixty-four. I don't expect you to multiply numbers as large as those in your head, but you might add five and six to get eleven, then happen to know that two to the eleven is two thousand and forty-eight. (Having a head full of powers of two is an occupational hazard of programming computers.) Why does that trick work?

Exactly one of five and six is an odd number, and so is eleven. That's odd! It may seem trivial, but at least one of five and six is positive (as opposed to zero), and eleven is positive too. (The only way the sum of two counts can be zero is if they were both separately zero.)

My point is that
\[
5 + 6 \,=\, 11
\]
is not a random isolated truth. It has a relationship with a whole bunch of other truths about other senses of ``combining stuff'', which all go
\[
\mbox{Combining the fivey thing with the sixy thing gives you the eleveny thing.}
\]
and of course, five, six and eleven are overly concrete examples---what matters is that they are related by a truth about adding numbers, a truth which somehow survives the translation to a truth about combining $n$y things, be they toe-taps, bags of spuds, or tests for oddness. Moreover, it wasn't only the things which mattered---it was also how they were combined. When we doubled we got a truth about adding, but when we two-to-the-powered we got a truth about multiplying, and when we tested for positivity we got a truth about ``either or both''.


\section{pictures of combining}

\newcommand{\wye}[3]{\begin{array}[b]{@{}c@{}}\underbrace{\!\!#1\;\;#2\!\!}\\ #3\end{array}}
\newcommand{\wir}[2]{\begin{array}[b]{@{}c@{}}#1\\ \mid\\ #2\end{array}}
\newcommand{\wid}[1]{\wir{#1}{#1}}
\newcommand{\naw}[1]{\begin{array}[b]{@{}c@{}}\bullet \vspace*{-0.13in}\\ \mid \\ #1\end{array}}

We can draw pictures of strategies for combining things like this
\[
\wye{5}{6}{11}
\]
Data flows from top to bottom. This component
\[
\wye\downarrow\downarrow\downarrow
\]
has two inputs and one output. We can wire these components together to combine more things.
\[
\wye{\wye\downarrow\downarrow\mid}\downarrow\downarrow
\]
If we choose what sort of things we're combining (numbers, say) and how (adding, say), then by labelling the wires with data, we can make assertions about the results of combining things. I usually just write the data instead of the wire, so
\[
\wye{\wye{5}{6}{11}}{7}{18}
\qquad\mbox{asserts}\qquad
5 + 6 \,=\, 11\quad\mbox{and}\quad 11 + 7 \,=\, 18
\]
but it's allowed to label the same wire more than once as long as you label it with the same thing. Signals don't change value in the middle of a wire. The same calculation is indicated by
\[
\wye{\wye{5}{6}{11}}{\wid{7}}{\wid{18}}
\]

Now, some notions of combining things are equipped with ``the sensible way to do nothing''. That's given by a component with no inputs and one output
\[
\naw{\downarrow}
\qquad\mbox{such that}\qquad
\wye{\naw{}}\cdot\cdot \;=\; \wid\cdot \;=\;\wye\cdot{\naw{}}\cdot
\]
In other words, combining with nothing turns into wire: that the output must be the same as the input is what ``doing nothing'' means.

For adding numbers, 0 is the right way to do nothing,
\[
\naw{0}
\qquad\mbox{so for any $n$,}\quad
\wye{\naw{0}}nn \;=\; \wid n \;=\;\wye n{\naw{0}}n
\]
It's very useful to have a sensible way to do nothing. That way, we can look out of an aeroplane window and count the fish, or say how many spuds we have left once we've eaten them all. Our rules tell us that there's only one sensible way to do nothing for any given way of combining. Imagine we had
\[
\naw{a} \; \mbox{and}\; \naw{b}
\qquad\mbox{then}\qquad
\wir bc\,=\,\wye{\naw{a}}{\naw{b}}c\,=\,\wir ac
\qquad\mbox{so}\qquad a = c = b
\]
That is, on the left, we do the left nothing to the right nothing, and on the right, we do the right nothing to the left nothing: our rules tell us we must end up with the same nothing. So if you see a ``way of combining'', \emph{look} for ``the sensible way to do nothing''---you won't find two, you might find one, and if there's no sensible way to do nothing, that's interesting, too.

When we're combining a bunch of stuff, we are sometimes in the lucky position that it's the sequence of \emph{the stuff} which determines the result of combining them, not the structure of pairwise combinations we choose. If we add up the list $[5,6,7]$, we get $18$, whether we calculate it this way
\[
\wye{\wye56{11}}7{18}
\qquad\mbox{or this way}\qquad
\wye5{\wye67{13}}{18}
\qquad\mbox{or even this way}\qquad
\wye5{\wye6{\wye7{\naw0}7}{13}}{18}
\]

In general, we want
\[
\wye{\wye\cdot\cdot\mid}\cdot\cdot \;=\; \wye\cdot{\wye\cdot\cdot\mid}\cdot
\]
so that we can regroup the calculation without reordering the inputs and be sure of getting the same output (even though the intermediate data change). That's why we don't bother writing brackets in $5+6+7$. You can combine data onto a ``running total'' one at a time, or you can give half the data to a friend and combine your outputs afterwards. Knowing what doesn't matter makes it far easier to gain confidence about what does!

Of course, I haven't really demonstrated that only the sequence of the inputs determines the outputs, not the structure of the combination tree. Let me sketch one way to see it. Let me say that one of our diagrams is \emph{listy} if it is always overbalanced as far to the right as possible, and has a nothing in the top right corner, i.e.
\begin{itemize}
\item inputs occur only on the left of $\wye\cdot\cdot\cdot$
\item only inputs occur on the left of $\wye\cdot\cdot\cdot$
\end{itemize}
Think carefully about the difference in meaning made by the difference in word order. The first condition rules out
\[
\wid 6 \quad\mbox{and}\quad \wye56{11}
\]
in both cases because $6$ is somewhere it is not permitted.
The second condition rules out
\[
\wye{\naw0}66 \quad\mbox{and}\quad \wye{\wye56{11}}7{18}
\]
because some left things are not inputs.

Each of our sequences of inputs can be put into a listy diagram because
\begin{itemize}
\item if you have no inputs, only $\naw\cdot$ is listy;
\item if you have a first input $x$, you must make $\wye xLt$ where $L$ is the listy diagram made with all but the first input.
\end{itemize}
But that's not enough, because we need to know that \emph{any} diagram can be transformed into its listy counterpart by using the three rules we gave ourselves
\[
\wye{\naw{}}\cdot\cdot \;=\; \wid\cdot \;=\;\wye\cdot{\naw{}}\cdot \qquad
\wye{\wye\cdot\cdot\mid}\cdot\cdot \;=\; \wye\cdot{\wye\cdot\cdot\mid}\cdot
\]
Here's how:
\begin{itemize}
\item $\naw{}$ is already listy;
\item $\wid\cdot$ can be make listy like this $\wye\cdot{\naw{}}\cdot$;
\item if you have a diagram $\wye{D_0}{D_1}t$, you can make it listy by first making $D_0$ into listy $L_0$ and then making listy $D_1$ into listy $L_1$, so you have $\wye{L_0}{L_1}t$. Now \emph{rotate} the whole thing into being listy like this:
  \begin{itemize}
  \item if you have $\wye{\naw{}}{L_1}t$, turn it into $L_1$ (whose `total' must already be $t$);
  \item if you have $\wye{\wye x{L'_0}\mid}{L_1}t$ rotate it to $\wye x{\wye{L'_0}{L_1}\mid}t$, then keep rotating on the right.
  \end{itemize}
\end{itemize}
So we've used all and only the rules we asked for. The rotation process effectively pastes $L_1$ over the $\naw{}$ in the top right corner of $L_0$.

There are lots of things to say about this ``explanation'', some good, some bad. Does it make sense to you? How would you check it?

How do you know, for example, that ``then keep rotating on the right'' actually achieves something other than endless rotation? There's a sense that rotation of $\wye{L_0}{L_1}t$ works by looking at $L_0$, and if $L_0 = \wye x{L'_0}\mid$, then we keep rotating with $\wye{L'_0}{L_1}\mid$, where the new diagram on the left, $L'_0$ is smaller than the old diagram on the left $L_0$. As long as we can't keep cutting smaller diagrams out of bigger ones forever, we'll be fine---sooner or later, we'll hit $\naw{}$. But is that explanation of the explanation just more waffle, or is it something we can codify?

Another potential grumble is that the whole thing depends on inventing this notion of which diagrams are ``listy'', and it looks like I just plucked that definition out of my arse. Where did it really come from?

As for the positives, for one thing, no numbers got added in the course of this narrative. We said which three rules we needed, and we deduced an ``only the sequence of inputs determines the output'' result for \emph{any} notion of combining things which obeys those rules. If we want the same result for multiplication (where 1 is ``the right way of doing nothing''), we know what we need to check.

Moreover, our story about pictures of combining things involved inventing a way to combine \emph{listy pictures} of combining \emph{things}. To combine $L_0$ and $L_1$, paste $L_1$ over the $\naw{}$ in the top right corner of $L_0$, or in other words, combine the sequences by concatenating them---joining them up end-of-one-to-start-of-the-next in space. Here $\naw{}$ is the ``right way of doing nothing'', as it represents the empty sequence. 

For example, here are the listy pictures for combining five things and six things, respectively:
\newcommand{\mo}[1]{\wye{\cdot}{#1}{\mid}}
\[
\wye{\cdot}{\mo{\mo{\mo{\mo{\naw{}}}}}}{\cdot}
\qquad\qquad
\wye{\cdot}{\mo{\mo{\mo{\mo{\mo{\naw{}}}}}}}{\cdot}
\]
and if you combine them, you get
\[
\wye{\cdot}{\mo{\mo{\mo{\mo{\mo{\mo{\mo{\mo{\mo{\mo{\naw{}}}}}}}}}}}}{\cdot}
\]
which is, of course, the listy picture for combining eleven things, so perhaps numbers \emph{did} get added, after all.


\section{it's called a ``monoid''}

While I'm trying to reduce the number of words by drawing more pictures, we are going to need a name for these ``ways of combining a bunch of stuff''. When I do introduce terminology, I'll try to make it the standard terminology. The standard name is ``monoid'', which is Greek for ``one-ish'', as in ``you can take any sequence of things and combine them into one thing''. Let's review the definition.

\newcommand{\neu}{\varepsilon}
\newcommand{\com}{\circ}
A \textbf{monoid} is given by a type $T$, a value $\neu$ in $T$, and an operator $\com$ which takes two inputs from $T$ and yields one output in $T$, satisfying the laws
\[
\neu\com t = t \qquad
t\com\neu = t \qquad
(r\com s)\com t = r\com(s\com t)
\]
We've been drawing
\[
\naw\neu \qquad \wye st{s\com t}
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{solutions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{sol}{Solution}[section]

\section{spuds and lemons}

\begin{sol}
  An `add-respects-or' test is a computable way to decide a true-or-false property of spuds-and-lemon numbers such that
  \begin{itemize}
  \item the test gives false for a $\lemon$;
    \item whenever you can split up any number $x$ as $y+z$ in any way, the test gives true for $x$ if and only if the test gives true either for $y$ or for $z$ or for both.
  \end{itemize}
  A `boring' test is one which always gives the same answer, no matter what number you test.
  \begin{enumerate}
  \item Find one boring add-respects-or test.\\
    \emph{The test which always gives false is certainly boring, and it satisfies both conditions for being add-respects-or.}
  \item Either find a different boring add-respects-or test, or explain why no such test exists.\\
    \emph{The only other boring test is the one which always gives true, but it violates the condition that the test must give false for the $\lemon$.}
  \item Find an add-respects-or test which is \emph{not} boring.
    \emph{The property of containing a $\spud\;$ is false for $\lemon$ but true for all the other numbers, so it is not boring and it satisifies the first criterion. Addition exactly preserves all the $\spud\,$s in the numbers it acts on, so the second condition is satsfied, too.}
  \item Either find a different non-boring add-respects-or test, or explain why no such test exists.   \emph{Every number can be decomposed as the sum of as many $\spud\lemon$ numbers as there were $\spud\,$s in the first place. As a result, an add-respects-or test for any number with spuds gives the same answer as testing $\spud\lemon$: if that's false, it's boring; if that's true, that's testing for containing a spud; other options are available. }
  \end{enumerate}
\end{sol}

\begin{sol}
  Let us say that a number is `munky' if it is exactly divisible by three, `minky' if division by three leaves a remainder of one, and `manky' if division by three leaves a remainder of two. Explain how to test these properties of spuds-and-lemon numbers. You should be able to remove one spud at a time and test what is left. After all, you will eventually get to the lemon! Hint: it is easier to solve all three parts of this puzzle together, rather than taking the one at a time.

  \emph{If you're testing a $\lemon$, it's munky, not minky or manky. Meanwhile $\spud n$ is munky if $n$ is manky, minky if $n$ is munky, and manky if $n$ is minky.}
\end{sol}


\begin{sol}
  Our method of addition replaced the left $\lemon$ by the right number. You could also consider computations which replace each $\spud$ in a number by the same something (which had better be ready to attach to a number). What can you compute in this way?

  \emph{Multiplication. If you have left and right numbers, removing the right lemon gives you something to replace each left spud with. That will add as many copies of the right number as there are left spuds. }
\end{sol}


\begin{sol}
  Our method of addition dismantles the left number and keeps the right number intact. It is thus far from obvious that $x+y = y+x$. Devise a method for adding in a way which is conspicuously symmetrical. Do the same for multiplication.

  \emph{You can tear down both numbers at the same time. If either number is a $\lemon$, their sum is the other number. If they're both $\spud\,$s, remove both outermost spuds, add what remains, then attach two spuds to the answer. The key to multiplication is to add at the same time. Here goes. If either number is a $\lemon$, their sum is the other number and their product is a $\lemon$. If they're both $\spud\,$s, remove both outermost spuds, add and multiply what remains; grow the product by the sum and one spud, and grow the sum by two spuds as before.}
\end{sol}

\begin{sol}
  A `multiply-respects-or' test is a computable way to decide a true-or-false property of spuds-and-lemon numbers such that
  \begin{itemize}
  \item the test gives false for $\spud\lemon$;
  \item whenever you can split up any number $x$ as $y\times z$ in any way, the test gives true for $x$ if and only if the test gives true either for $y$ or for $z$ or for both.
  \end{itemize}
  \begin{enumerate}
  \item Are any of your `minky', `manky', `munky' tests multiply-respects-or? If so, which and why?\\
    \emph{Testing for being munky is multiply-respects-or, because $3$ is prime.
    Factorising a number amounts to partitioning its prime factors. A $3$ in the whole set of prime factors must end up in one part of the partition. $4$ is a minky number, but $4 = 2*2$ and $2$ is manky, so neither minky nor manky are multiply-respects-or tests.}
  \item In a black box, I have a multiply-respects-or test which is not boring. What answer does it give for the $\lemon$? No, you may not look in the box!\\
    \emph{It must give true. Suppose otherwise. Pick any number $n$. Recall that $n*0 = 0$.
    That means testing $n$ or false gives false, by respecting or, so testing $n$ gives false. Hence the test is boring.}
  \item Is divisibility by \emph{four} a multiply-respects-or test? Why?\\
    \emph{No. $4 = 2*2$, but neither $2$ nor $2$ is divisible by $4$, and $4$ is divisible by $4$, so multiply does not respect or.}
  \item What are the non-boring multiply-respects-or tests? (Note: it's terribly unfair of me to ask this question at this stage, but we'll make sense of it eventually.)\\
    \emph{Every number has a unique factorisation into primes, so any multiply-respects-or test is determined exactly by its behaviour on prime numbers. Such a test determines the subset $S$ of prime numbers for which it returns true, and it will thus return true exactly for any number with a factor in $S$. Every subset of the primes gives the $S$ for such a test, and it is only boring when $S$ is empty.}
  \end{enumerate}
\end{sol}



\end{document}
